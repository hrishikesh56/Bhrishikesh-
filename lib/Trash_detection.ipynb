{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "741b5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import skimage\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from retinanet import model\n",
    "from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, \\\n",
    "    Normalizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from retinanet import coco_eval\n",
    "from retinanet import csv_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d3e1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'trash_trained/final_model.pt'\n",
    "model = torch.load(model_path,map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b7731f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_dict = {};\n",
    "system_dict[\"verbose\"] = 1;\n",
    "system_dict[\"local\"] = {};\n",
    "system_dict[\"local\"][\"common_size\"] = 512;\n",
    "system_dict[\"local\"][\"min_side\"] = 608;\n",
    "system_dict[\"local\"][\"max_side\"] = 1024;\n",
    "system_dict[\"local\"][\"mean\"] = np.array([[[0.485, 0.456, 0.406]]]);\n",
    "system_dict[\"local\"][\"std\"] = np.array([[[0.229, 0.224, 0.225]]]);\n",
    "system_dict[\"local\"][\"colors\"] = [(39, 129, 113), (164, 80, 133), (83, 122, 114), (99, 81, 172), (95, 56, 104), (37, 84, 86), (14, 89, 122),\n",
    "    (80, 7, 65), (10, 102, 25), (90, 185, 109), (106, 110, 132), (169, 158, 85), (188, 185, 26), (103, 1, 17),\n",
    "    (82, 144, 81), (92, 7, 184), (49, 81, 155), (179, 177, 69), (93, 187, 158), (13, 39, 73), (12, 50, 60),\n",
    "    (16, 179, 33), (112, 69, 165), (15, 139, 63), (33, 191, 159), (182, 173, 32), (34, 113, 133), (90, 135, 34),\n",
    "    (53, 34, 86), (141, 35, 190), (6, 171, 8), (118, 76, 112), (89, 60, 55), (15, 54, 88), (112, 75, 181),\n",
    "    (42, 147, 38), (138, 52, 63), (128, 65, 149), (106, 103, 24), (168, 33, 45), (28, 136, 135), (86, 91, 108),\n",
    "    (52, 11, 76), (142, 6, 189), (57, 81, 168), (55, 19, 148), (182, 101, 89), (44, 65, 179), (1, 33, 26),\n",
    "    (122, 164, 26), (70, 63, 134), (137, 106, 82), (120, 118, 52), (129, 74, 42), (182, 147, 112), (22, 157, 50),\n",
    "    (56, 50, 20), (2, 22, 177), (156, 100, 106), (21, 35, 42), (13, 8, 121), (142, 92, 28), (45, 118, 33),\n",
    "    (105, 118, 30), (7, 185, 124), (46, 34, 146), (105, 184, 169), (22, 18, 5), (147, 71, 73), (181, 64, 91),\n",
    "    (31, 39, 184), (164, 179, 33), (96, 50, 18), (95, 15, 106), (113, 68, 54), (136, 116, 112), (119, 139, 130),\n",
    "    (31, 139, 34), (66, 6, 127), (62, 39, 2), (49, 99, 180), (49, 119, 155), (153, 50, 183), (125, 38, 3),\n",
    "    (129, 87, 143), (49, 87, 40), (128, 62, 120), (73, 85, 148), (28, 144, 118), (29, 9, 24), (175, 45, 108),\n",
    "    (81, 175, 64), (178, 19, 157), (74, 188, 190), (18, 114, 2), (62, 128, 96), (21, 3, 150), (0, 6, 95),\n",
    "    (2, 20, 184), (122, 37, 185)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a57c09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'img1.jpg'\n",
    "image_filename = os.path.basename(img_path)\n",
    "img = skimage.io.imread(img_path)\n",
    "image = img.astype(np.float32) / 255.;\n",
    "image = (image.astype(np.float32) - system_dict[\"local\"][\"mean\"]) / system_dict[\"local\"][\"std\"];\n",
    "\n",
    "rows, cols, cns = image.shape\n",
    "\n",
    "smallest_side = min(rows, cols)\n",
    "\n",
    "# rescale the image so the smallest side is min_side\n",
    "scale = system_dict[\"local\"][\"min_side\"] / smallest_side\n",
    "\n",
    "# check if the largest side is now greater than max_side, which can happen\n",
    "# when images have a large aspect ratio\n",
    "largest_side = max(rows, cols)\n",
    "\n",
    "if largest_side * scale > system_dict[\"local\"][\"max_side\"]:\n",
    "    scale = system_dict[\"local\"][\"max_side\"]  / largest_side\n",
    "\n",
    "\n",
    "# resize the image with the computed scale\n",
    "image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale)))))\n",
    "rows, cols, cns = image.shape\n",
    "\n",
    "pad_w = 32 - rows%32\n",
    "pad_h = 32 - cols%32\n",
    "\n",
    "new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)\n",
    "new_image[:rows, :cols, :] = image.astype(np.float32)\n",
    "\n",
    "img = torch.from_numpy(new_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f63ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca15bbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da854d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d1480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d97d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep_learning)",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
